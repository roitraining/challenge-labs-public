{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8a427b8-67f5-4ee1-a33e-24e872359082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b9eceb7-f314-4546-9149-1604a10c564e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 19:01:27.878260: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-17 19:01:27.880287: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-17 19:01:27.889403: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-17 19:01:27.908994: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-17 19:01:27.937191: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-17 19:01:27.945241: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-17 19:01:27.971898: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import joblib\n",
    "import dill\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed304cc8-ec58-4536-b2ee-17c110e8c07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT = \"jwd-test-sbcl\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "MODEL_NAME = \"adult-income-cpr-model\"\n",
    "BUCKET_NAME = \"jwd-test-sbcl\"\n",
    "GCS_FOLDER = \"adult-income-cpr-model\"\n",
    "\n",
    "MODEL_LOCAL_PATH=\"./adult-income-cpr-model\"\n",
    "SRC_LOCAL_PATH=\"./source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806274de-7958-4aed-bfed-f74b00fdb363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_LOCAL_PATH, exist_ok=True)\n",
    "os.makedirs(SRC_LOCAL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7439d46-f717-46a5-9ad1-a5dbdc458cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./adult-income-cpr-model/categorical_encoder.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('adult-income.csv')\n",
    "\n",
    "# Exclude 'functional_weight' and 'income_bracket' from features\n",
    "features = ['age', 'workclass', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "            'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "X = data[features].values\n",
    "y = data['income_bracket'].values\n",
    "\n",
    "# Encode the string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = ['workclass', 'education', 'marital_status', 'occupation', \n",
    "                        'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "# OneHotEncode the categorical features\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False)\n",
    "categorical_encoded = categorical_encoder.fit_transform(data[categorical_features])\n",
    "\n",
    "# Combine the numerical features with the encoded categorical features\n",
    "numerical_features = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "X_combined = np.hstack((data[numerical_features].values, categorical_encoded))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numerical features in the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[:, :len(numerical_features)])\n",
    "X_test_scaled = scaler.transform(X_test[:, :len(numerical_features)])\n",
    "\n",
    "# Combine the scaled numerical features with the encoded categorical features\n",
    "X_train_final = np.hstack((X_train_scaled, X_train[:, len(numerical_features):]))\n",
    "X_test_final = np.hstack((X_test_scaled, X_test[:, len(numerical_features):]))\n",
    "\n",
    "# Save the scaler and encoders for later use during prediction\n",
    "joblib.dump(scaler, f'{MODEL_LOCAL_PATH}/scaler.pkl')\n",
    "joblib.dump(label_encoder, f'{MODEL_LOCAL_PATH}/label_encoder.pkl')\n",
    "joblib.dump(categorical_encoder, f'{MODEL_LOCAL_PATH}/categorical_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e049af-d8f3-46bc-8586-d4ffa3facef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t = joblib.load(f'{MODEL_LOCAL_PATH}/scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "513943c8-2aee-48dc-9889-078f8b961c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m652/652\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8209 - loss: 0.3717 - val_accuracy: 0.8511 - val_loss: 0.3328\n",
      "2.17.0\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_final.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_final, y_train, epochs=1, validation_split=0.2)\n",
    "\n",
    "model.save(f\"{MODEL_LOCAL_PATH}/{MODEL_NAME}.keras\")\n",
    "# tf.keras.models.save_model(model, f\"{MODEL_LOCAL_PATH}/{MODEL_NAME}.keras\")\n",
    "\n",
    "t_model = tf.keras.models.load_model(f\"{MODEL_LOCAL_PATH}/{MODEL_NAME}.keras\")\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cab53a5c-6216-4f19-95a3-0bd5521442d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded ./adult-income-cpr-model/adult-income-cpr-model.keras to gs://jwd-test-sbcl/adult-income-cpr-model/adult-income-cpr-model.keras\n",
      "Uploaded ./adult-income-cpr-model/categorical_encoder.pkl to gs://jwd-test-sbcl/adult-income-cpr-model/categorical_encoder.pkl\n",
      "Uploaded ./adult-income-cpr-model/scaler.pkl to gs://jwd-test-sbcl/adult-income-cpr-model/scaler.pkl\n",
      "Uploaded ./adult-income-cpr-model/label_encoder.pkl to gs://jwd-test-sbcl/adult-income-cpr-model/label_encoder.pkl\n"
     ]
    }
   ],
   "source": [
    "# Upload the model to Google Cloud Storage\n",
    "def upload_directory_to_gcs(local_directory, bucket_name, gcs_destination):\n",
    "    \"\"\"\n",
    "    Uploads a local directory to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        local_directory (str): Path to the local directory.\n",
    "        bucket_name (str): Name of the GCS bucket.\n",
    "        gcs_destination (str): GCS destination path.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            gcs_path = os.path.join(gcs_destination, relative_path)\n",
    "\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{gcs_path}\")\n",
    "\n",
    "# Call the function to upload the directory\n",
    "upload_directory_to_gcs(MODEL_LOCAL_PATH, BUCKET_NAME, GCS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef2a4295-2d29-488c-808b-fe5a083b5add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./source/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SRC_LOCAL_PATH/requirements.txt\n",
    "fastapi\n",
    "uvicorn\n",
    "pandas\n",
    "tensorflow\n",
    "google-cloud-storage\n",
    "google-cloud-aiplatform[prediction]\n",
    "scikit-learn\n",
    "dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2c4842d-9e1e-4f4d-bab5-4d46e2b7bf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./source/predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SRC_LOCAL_PATH/predictor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from typing import Dict\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "MODEL_NAME = \"adult-income-cpr-model\"\n",
    "\n",
    "class CustomPredictor(Predictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def load(self, artifacts_uri: str):\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        files_and_dirs = os.listdir('.')\n",
    "        for item in files_and_dirs:\n",
    "            print(item)\n",
    "        print(tf.__version__)\n",
    "        temp = tf.keras.models.load_model(f\"{MODEL_NAME}.keras\")\n",
    "        \n",
    "#         try:\n",
    "#             print('doing scaler')\n",
    "#             with open(f\"scaler.pkl\", \"rb\") as f:\n",
    "#                 scaler = joblib.load(f)\n",
    "#             self._scaler = scaler\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "    \n",
    "#         try:\n",
    "#             print('doing label')\n",
    "#             with open(f\"label_encoder.pkl\", \"rb\") as f:\n",
    "#                 label_encoder = joblib.load(f)\n",
    "#             self._label_encoder = label_encoder\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "        \n",
    "#         try:\n",
    "#             print('doing categories')\n",
    "#             with open(f\"categorical_encoder.pkl\", \"rb\") as f:\n",
    "#                 categorical_encoder = joblib.load(f)\n",
    "#             self._categorical_encoder = categorical_encoder\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "        \n",
    "#         try:\n",
    "#             print('doing model - take 1')\n",
    "#             self._model = tf.keras.models.load_model(f\"{MODEL_NAME}.keras\")\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "        \n",
    "\n",
    "    def preprocess(self, prediction_input):\n",
    "        instances = prediction_input\n",
    "        # instances_numeric_features = np.array([[instance[0], instance[3], instance[9], instance[10], instance[11]] for instance in instances])\n",
    "        # instances_scaled_numeric_features = scaler.transform(instances_numeric_features)        \n",
    "        # instances_categorical_features = np.array([instance[1:3] + instance[4:9] + [instance[12]] for instance in instances])\n",
    "        # instances_categorical_encoded = categorical_encoder.transform(instances_categorical_features)\n",
    "        # instances_combined = np.hstack((instances_scaled_numeric_features, instances_categorical_encoded))\n",
    "        # preprocessed_instances_as_list = instances_combined.tolist()\n",
    "        # return preprocessed_instances_as_list\n",
    "        return prediction_input\n",
    "\n",
    "    def predict(self, instances):\n",
    "        return self._model.predict(instances=instances)\n",
    "\n",
    "    def postprocess(self, prediction_results):\n",
    "        output = []\n",
    "        for prediction in response.predictions:\n",
    "            predicted_label = np.argmax(prediction)\n",
    "            decoded_label = label_encoder.inverse_transform([predicted_label])[0]\n",
    "            output_entry = {\n",
    "                \"predicted_label\": deocded_label,\n",
    "                \"predicted_probabilities\": prediction\n",
    "            }\n",
    "            output.append(output_entry)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84bfb030-91e5-497a-8d44-9a28cbe1f576",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/subprocess.py:955: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdin = io.open(p2cwrite, 'wb', bufsize)\n",
      "/opt/conda/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from source.predictor import CustomPredictor\n",
    "\n",
    "REPOSITORY = \"adult-income-cpr-repo\"  # @param {type:\"string\"}\n",
    "IMAGE = \"adult-income-cpr-server\"  # @param {type:\"string\"}\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    SRC_LOCAL_PATH,\n",
    "    f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{IMAGE}\",\n",
    "    predictor=CustomPredictor,\n",
    "    requirements_path=os.path.join(SRC_LOCAL_PATH, \"requirements.txt\"),\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2914e6e-2cb0-47c2-be8c-779c1441e78e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image_uri: \"us-central1-docker.pkg.dev/jwd-test-sbcl/adult-income-cpr-repo/adult-income-cpr-server\"\n",
       "predict_route: \"/predict\"\n",
       "health_route: \"/health\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_model.get_serving_container_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db58b33a-1724-4176-bb66-dcf28b61b484",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]> b'{}'\n",
      "<Response [400]> b'{\"detail\":\"Unsupported content type of the request: None.\\\\nCurrently supported content-type in DefaultSerializer: \\\\\"application/json\\\\\".\"}'\n"
     ]
    }
   ],
   "source": [
    "request = \"\"\"\n",
    "[\n",
    "    [39,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Other-service\",\"Wife\",\"Black\",\"Female\",3411,0,34,\"United-States\"],\n",
    "    [77,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Priv-house-serv\",\"Wife\",\"Black\",\"Female\",0,0,10,\"United-States\"],\n",
    "    [27,\"Local-gov\",\"HS-grad\",9,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,80,\"United-States\"],\n",
    "    [40,\"Private\",\"Masters\",14,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,46,\"United-States\"]\n",
    "\n",
    "]\n",
    "\"\"\"\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=f\"gs://{BUCKET_NAME}/{GCS_FOLDER}\"\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    print(health_check_response, health_check_response.content)\n",
    "\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=request,\n",
    "        headers={\"header-key\": \"header-value\"},\n",
    "    )\n",
    "    print(predict_response, predict_response.content)\n",
    "\n",
    "    local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374fb2fa-5616-4343-8390-69eda5d4fc04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import artifactregistry_v1\n",
    "from google.cloud.artifactregistry_v1 import Repository\n",
    "from google.api_core.operation import Operation\n",
    "\n",
    "def create_gcloud_repository(repository, region):\n",
    "    client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "    parent = f\"projects/{PROJECT}/locations/{region}\"\n",
    "\n",
    "    repo = Repository(\n",
    "        name=f\"{parent}/repositories/{repository}\",\n",
    "        format_=artifactregistry_v1.Repository.Format.DOCKER\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        operation: Operation = client.create_repository(\n",
    "            parent=parent,\n",
    "            repository_id=repository,\n",
    "            repository=repo\n",
    "        )\n",
    "        print(\"Waiting for operation to complete...\")\n",
    "        response = operation.result()  # Wait for the operation to complete\n",
    "        print(f\"Repository created successfully: {response.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "\n",
    "create_gcloud_repository(REPOSITORY, REGION)\n",
    "local_model.push_image()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1428d015-6600-4af7-bebb-293f0d4348d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "aiplatform.init(project=PROJECT, location=REGION)\n",
    "model = aiplatform.Model.upload(\n",
    "    local_model=local_model,\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=f\"gs://{BUCKET_NAME}/{GCS_FOLDER}\",\n",
    ")\n",
    "endpoint = model.deploy(machine_type=\"n1-standard-4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c040d2e-def8-41e7-98a6-e16966999da2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "health_check_response = le.run_health_check()\n",
    "print(health_check_response, health_check_response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0dd08-1b3b-4141-9002-11ed8b6d605f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile instances.json\n",
    "[\n",
    "    [39,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Other-service\",\"Wife\",\"Black\",\"Female\",3411,0,34,\"United-States\"],\n",
    "    [77,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Priv-house-serv\",\"Wife\",\"Black\",\"Female\",0,0,10,\"United-States\"],\n",
    "    [27,\"Local-gov\",\"HS-grad\",9,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,80,\"United-States\"],\n",
    "    [40,\"Private\",\"Masters\",14,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,46,\"United-States\"]\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89d8ec-6074-400c-b874-ec2e776df8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_response = le.predict(\n",
    "        request_file=INPUT_FILE,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8cbb5-d159-4557-832a-ddfd36384b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = [\n",
    "    [39,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Other-service\",\"Wife\",\"Black\",\"Female\",3411,0,34,\"United-States\"],\n",
    "    [77,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Priv-house-serv\",\"Wife\",\"Black\",\"Female\",0,0,10,\"United-States\"],\n",
    "    [27,\"Local-gov\",\"HS-grad\",9,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,80,\"United-States\"],\n",
    "    [40,\"Private\",\"Masters\",14,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,46,\"United-States\"]\n",
    "\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "print(response.predictions)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
