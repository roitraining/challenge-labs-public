{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a427b8-67f5-4ee1-a33e-24e872359082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9eceb7-f314-4546-9149-1604a10c564e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import joblib\n",
    "import dill\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed304cc8-ec58-4536-b2ee-17c110e8c07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set required constants (replace project with your project id)\n",
    "\n",
    "PROJECT = \"[project]\"\n",
    "LOCATION = \"us-central1\"\n",
    "\n",
    "MODEL_NAME = \"adult-income-cpr-model\"\n",
    "\n",
    "MODEL_LOCAL_PATH=\"./adult-income-cpr-model\"\n",
    "SRC_LOCAL_PATH=\"./source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806274de-7958-4aed-bfed-f74b00fdb363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build local directories\n",
    "\n",
    "os.makedirs(MODEL_LOCAL_PATH, exist_ok=True)\n",
    "os.makedirs(SRC_LOCAL_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7439d46-f717-46a5-9ad1-a5dbdc458cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('adult-income.csv')\n",
    "\n",
    "# Exclude 'functional_weight' and 'income_bracket' from features\n",
    "features = ['age', 'workclass', 'education', 'education_num', 'marital_status', 'occupation', \n",
    "            'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country']\n",
    "X = data[features].values\n",
    "y = data['income_bracket'].values\n",
    "\n",
    "# Encode the string labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = ['workclass', 'education', 'marital_status', 'occupation', \n",
    "                        'relationship', 'race', 'sex', 'native_country']\n",
    "\n",
    "# OneHotEncode the categorical features\n",
    "categorical_encoder = OneHotEncoder(sparse_output=False)\n",
    "categorical_encoded = categorical_encoder.fit_transform(data[categorical_features])\n",
    "\n",
    "# Combine the numerical features with the encoded categorical features\n",
    "numerical_features = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n",
    "X_combined = np.hstack((data[numerical_features].values, categorical_encoded))\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the numerical features in the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train[:, :len(numerical_features)])\n",
    "X_test_scaled = scaler.transform(X_test[:, :len(numerical_features)])\n",
    "\n",
    "# Combine the scaled numerical features with the encoded categorical features\n",
    "X_train_final = np.hstack((X_train_scaled, X_train[:, len(numerical_features):]))\n",
    "X_test_final = np.hstack((X_test_scaled, X_test[:, len(numerical_features):]))\n",
    "\n",
    "# Save the scaler and encoders for later use during prediction\n",
    "joblib.dump(scaler, f'{MODEL_LOCAL_PATH}/scaler.pkl')\n",
    "joblib.dump(label_encoder, f'{MODEL_LOCAL_PATH}/label_encoder.pkl')\n",
    "joblib.dump(categorical_encoder, f'{MODEL_LOCAL_PATH}/categorical_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513943c8-2aee-48dc-9889-078f8b961c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build, train and save the model\n",
    "\n",
    "# define the model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_final.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "model.fit(X_train_final, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# save the model\n",
    "model.save(f\"{MODEL_LOCAL_PATH}/{MODEL_NAME}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a4295-2d29-488c-808b-fe5a083b5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $SRC_LOCAL_PATH/requirements.txt\n",
    "fastapi\n",
    "uvicorn\n",
    "pandas\n",
    "tensorflow\n",
    "google-cloud-storage\n",
    "google-cloud-aiplatform[prediction]\n",
    "scikit-learn\n",
    "dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4842d-9e1e-4f4d-bab5-4d46e2b7bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $SRC_LOCAL_PATH/predictor.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from typing import Dict\n",
    "\n",
    "from google.cloud.aiplatform.prediction.predictor import Predictor\n",
    "from google.cloud.aiplatform.utils import prediction_utils\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "MODEL_NAME = \"adult-income-cpr-model\"\n",
    "\n",
    "class CustomPredictor(Predictor):\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    # load the model and the preprocessing objects\n",
    "    def load(self, artifacts_uri: str):\n",
    "        prediction_utils.download_model_artifacts(artifacts_uri)\n",
    "        self._model = tf.keras.models.load_model(f\"{MODEL_NAME}.keras\")\n",
    "        \n",
    "        with open(f\"scaler.pkl\", \"rb\") as f:\n",
    "            scaler = joblib.load(f)\n",
    "        self._scaler = scaler\n",
    "        \n",
    "        with open(f\"label_encoder.pkl\", \"rb\") as f:\n",
    "            label_encoder = joblib.load(f)\n",
    "        self._label_encoder = label_encoder\n",
    "\n",
    "        with open(f\"categorical_encoder.pkl\", \"rb\") as f:\n",
    "            categorical_encoder = joblib.load(f)\n",
    "        self._categorical_encoder = categorical_encoder\n",
    "            \n",
    "    # preprocess the raw input data\n",
    "    def preprocess(self, prediction_input):\n",
    "        instances = prediction_input[\"instances\"]\n",
    "        instances_numeric_features = np.array([[instance[0], instance[3], instance[9], instance[10], instance[11]] for instance in instances])\n",
    "        instances_scaled_numeric_features = self._scaler.transform(instances_numeric_features)        \n",
    "        instances_categorical_features = np.array([instance[1:3] + instance[4:9] + [instance[12]] for instance in instances])\n",
    "        instances_categorical_encoded = self._categorical_encoder.transform(instances_categorical_features)\n",
    "        instances_combined = np.hstack((instances_scaled_numeric_features, instances_categorical_encoded))\n",
    "        return instances_combined\n",
    "\n",
    "    # make the prediction\n",
    "    def predict(self, instances):\n",
    "         return self._model.predict(instances)\n",
    "\n",
    "    # select the higher probability\n",
    "    # convert to text label\n",
    "    # compose array of results\n",
    "    def postprocess(self, prediction_results):\n",
    "        predictions = []\n",
    "        for prediction in prediction_results:\n",
    "            predicted_label = np.argmax(prediction)\n",
    "            decoded_label = self._label_encoder.inverse_transform([predicted_label])[0]\n",
    "            output_entry = {\n",
    "                \"predicted_label\": decoded_label,\n",
    "                \"predicted_probabilities\": prediction.tolist()\n",
    "            }\n",
    "            predictions.append(output_entry)\n",
    "        return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfb030-91e5-497a-8d44-9a28cbe1f576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build a local model with custom predictor\n",
    "\n",
    "import importlib\n",
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "from source.predictor import CustomPredictor\n",
    "\n",
    "REPOSITORY = \"adult-income-cpr-repo\"  # @param {type:\"string\"}\n",
    "IMAGE = \"adult-income-cpr-server\"  # @param {type:\"string\"}\n",
    "\n",
    "local_model = LocalModel.build_cpr_model(\n",
    "    SRC_LOCAL_PATH,\n",
    "    f\"{LOCATION}-docker.pkg.dev/{PROJECT}/{REPOSITORY}/{IMAGE}\",\n",
    "    predictor=CustomPredictor,\n",
    "    requirements_path=os.path.join(SRC_LOCAL_PATH, \"requirements.txt\"),\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58b33a-1724-4176-bb66-dcf28b61b484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do the prediction\n",
    "\n",
    "import json\n",
    "\n",
    "request = \"\"\"\n",
    "    {\"instances\":[\n",
    "        [39,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Other-service\",\"Wife\",\"Black\",\"Female\",3411,0,34,\"United-States\"],\n",
    "        [77,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Priv-house-serv\",\"Wife\",\"Black\",\"Female\",0,0,10,\"United-States\"],\n",
    "        [27,\"Local-gov\",\"HS-grad\",9,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,80,\"United-States\"],\n",
    "        [40,\"Private\",\"Masters\",14,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,46,\"United-States\"]\n",
    "    ]}\n",
    "\"\"\"\n",
    "\n",
    "with local_model.deploy_to_local_endpoint(\n",
    "    artifact_uri=MODEL_LOCAL_PATH\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    print(health_check_response, health_check_response.content)\n",
    "\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=request,\n",
    "        headers={\"content-type\": \"application/json\"},\n",
    "    )\n",
    "    print(predict_response, predict_response.content)\n",
    "\n",
    "    local_endpoint.print_container_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5017dd-dc9d-4539-8b78-133643183cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# push the prediction container to Artifact Registry\n",
    "! gcloud auth configure-docker {LOCATION}-docker.pkg.dev --quiet\n",
    "! gcloud artifacts repositories create $REPOSITORY --repository-format=docker --location=$LOCATION --description=\"Docker repository\"\n",
    "local_model.push_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5af812-57f9-4b39-ad88-81b5e3501e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# upload the trained model to Vertex AI\n",
    "\n",
    "! gcloud storage cp {MODEL_NAME}/* gs://{PROJECT}/{MODEL_NAME}\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    local_model=local_model,\n",
    "    display_name=MODEL_NAME,\n",
    "    artifact_uri=f\"gs://{PROJECT}/adult-income-cpr-model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb50c6-0fbb-485b-9f92-ac166b5cf866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deploy the model to a new endpoint\n",
    "\n",
    "endpoint = model.deploy(machine_type=\"n2-highmem-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8b0360-9509-4413-940d-f2444ef3d2e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do predictions with the endpoint\n",
    "\n",
    "# populate the list of instances for prediction\n",
    "instances = [\n",
    "    [39,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Other-service\",\"Wife\",\"Black\",\"Female\",3411,0,34,\"United-States\"],\n",
    "    [77,\"Private\", \"9th\",5,\"Married-civ-spouse\",\"Priv-house-serv\",\"Wife\",\"Black\",\"Female\",0,0,10,\"United-States\"],\n",
    "    [27,\"Local-gov\",\"HS-grad\",9,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,80,\"United-States\"],\n",
    "    [40,\"Private\",\"Masters\",14,\"Married-civ-spouse\",\"Exec-managerial\",\"Husband\",\"White\",\"Male\",0,0,46,\"United-States\"]\n",
    "\n",
    "]\n",
    "\n",
    "payload = {\n",
    "    \"instances\":instances\n",
    "}\n",
    "\n",
    "response = endpoint.predict(instances=instances)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
