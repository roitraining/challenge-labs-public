{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 ROI Training, Inc. \n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic"
   },
   "source": [
    "# Vertex AI Pipelines: Automating Your AutoML Tabular Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:pipelines,automl",
    "tags": []
   },
   "source": [
    "- Create a pipeline that will:\n",
    "    - Create a dataset\n",
    "    - Train an AutoML classification model\n",
    "    - Create an endpoint\n",
    "    - Deploy your model\n",
    "    - Perform batch predictions\n",
    "- Compile the pipeline\n",
    "- Execute the KFP pipeline using **Vertex AI Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans"
   },
   "source": [
    "## 1. Prepare your environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_aip:mbsdk",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install required Python packages\n",
    "\n",
    "! pip3 install --quiet \\\n",
    "    google-cloud-storage \\\n",
    "    google-cloud-aiplatform \\\n",
    "    google-cloud-pipeline-components==2.4.1 \\\n",
    "    kfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "set_project_id",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set useful Python and environment variables\n",
    "\n",
    "import uuid\n",
    "\n",
    "# get info from gcloud\n",
    "proj_output     = !gcloud config get-value project\n",
    "sa_output       = !gcloud auth list 2>/dev/null\n",
    "\n",
    "# set variables for later use\n",
    "UUID            = uuid.uuid4().hex\n",
    "project         = proj_output[0]\n",
    "location        = \"us-central1\"\n",
    "service_account = sa_output[2].replace(\"*\", \"\").strip()\n",
    "bucket_uri      = f\"gs://{project}-pipelines\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the default region\n",
    "# Create buckets\n",
    "# Set permissions on the buckets\n",
    "\n",
    "! gcloud config set compute/region {location}\n",
    "\n",
    "! gsutil mb -l {location} -p {project} {bucket_uri}\n",
    "! gsutil iam ch serviceAccount:{service_account}:roles/storage.objectCreator $bucket_uri\n",
    "! gsutil iam ch serviceAccount:{service_account}:roles/storage.objectViewer $bucket_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:pipelines,automl,beans"
   },
   "source": [
    "## 2. Define the pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "define_pipeline:gcpc,beans,lcn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import uuid\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component, importer)\n",
    "\n",
    "# set variables to use when creating pipeline\n",
    "pipeline_name       = f\"pipeline-{UUID}\"\n",
    "pipeline_root       = f\"{bucket_uri}/pipeline_root/\"\n",
    "dataset_name        = f\"dataset_{UUID}\"\n",
    "train_job_name      = f\"train_job_{UUID}\"\n",
    "model_name          = f\"model_{UUID}\"\n",
    "endpoint_name       = f\"endpoint_{UUID}\"\n",
    "predict_job_name    = f\"predict_job_{UUID}\"\n",
    "machine_type        = \"n1-standard-4\"\n",
    "training_source     = \"gs://sb-challenge-labs/data_science/adult-income.csv\"\n",
    "prediction_source   = [\"gs://sb-challenge-labs/data_science/income-batch-pipelines.jsonl\"]\n",
    "prediction_target   = f\"{bucket_uri}/pipeline-predict\"\n",
    "\n",
    "@kfp.dsl.pipeline(name=pipeline_name, pipeline_root=pipeline_root)\n",
    "def pipeline(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    dataset_name: str,\n",
    "    train_job_name: str,\n",
    "    model_name: str,\n",
    "    endpoint_name: str,\n",
    "    predict_job_name: str,\n",
    "    machine_type: str,\n",
    "    training_source: str,\n",
    "    prediction_source: list,\n",
    "    prediction_target: str,\n",
    "):  \n",
    "    from google_cloud_pipeline_components.v1.automl.training_job import AutoMLTabularTrainingJobRunOp\n",
    "    from google_cloud_pipeline_components.v1.dataset.create_tabular_dataset.component import tabular_dataset_create as TabularDatasetCreateOp\n",
    "    from google_cloud_pipeline_components.v1.endpoint.create_endpoint.component import endpoint_create as EndpointCreateOp\n",
    "    from google_cloud_pipeline_components.v1.endpoint.deploy_model.component import model_deploy as ModelDeployOp\n",
    "    from google_cloud_pipeline_components.v1.batch_predict_job import ModelBatchPredictOp\n",
    "    \n",
    "    dataset_create_op = TabularDatasetCreateOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=dataset_name,\n",
    "        gcs_source=training_source\n",
    "    )\n",
    "\n",
    "    training_op = AutoMLTabularTrainingJobRunOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=train_job_name,\n",
    "        optimization_prediction_type=\"classification\",\n",
    "        budget_milli_node_hours=1000,\n",
    "        model_display_name=model_name,\n",
    "        dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "        target_column=\"income_bracket\"\n",
    "    )\n",
    "    \n",
    "    endpoint_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        display_name=endpoint_name,\n",
    "    )\n",
    "\n",
    "    deploy_op = ModelDeployOp(\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_machine_type=machine_type,\n",
    "    )\n",
    "\n",
    "    predict_op = ModelBatchPredictOp(\n",
    "        project=project,\n",
    "        location=location,\n",
    "        job_display_name=predict_job_name,\n",
    "        model=training_op.outputs[\"model\"],\n",
    "        instances_format=\"jsonl\",\n",
    "        predictions_format=\"jsonl\",\n",
    "        gcs_source_uris=prediction_source,\n",
    "        gcs_destination_output_uri_prefix=prediction_target\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## 3. Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "compile_pipeline",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile the pipeline with a package path of \"tabular_classification_pipeline.yaml\"\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"tabular_classification_pipeline.yaml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "compile_pipeline"
   },
   "source": [
    "## 4. Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_pipeline:model",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the connection and create the pipeline job object\n",
    "\n",
    "aiplatform.init(\n",
    "    project=project, \n",
    "    location=location, \n",
    "    staging_bucket=bucket_uri\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=pipeline_name,\n",
    "    template_path=\"tabular_classification_pipeline.yaml\",\n",
    "    pipeline_root=pipeline_root,\n",
    "    parameter_values={\n",
    "        \"project\": project,\n",
    "        \"location\": location,\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"train_job_name\": train_job_name,\n",
    "        \"model_name\": model_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"predict_job_name\": predict_job_name,\n",
    "        \"machine_type\": machine_type,\n",
    "        \"training_source\": training_source,\n",
    "        \"prediction_source\": prediction_source,\n",
    "        \"prediction_target\": prediction_target,\n",
    "    },\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "114ab8ff24ac"
   },
   "outputs": [],
   "source": [
    "# Run the job\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "automl_tabular_classification_beans.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m124",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m124"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
